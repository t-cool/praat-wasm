<html><head><meta name="robots" content="index,follow"><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>HMM & HMM: Get cross-entropy...</title>
<style>
   td { padding-left: 5pt; padding-right: 5pt; }
   th { padding-left: 5pt; padding-right: 5pt; }
   code { white-space: pre-wrap; }
   dd { white-space: pre-wrap; }
</style>
</head><body bgcolor="#FFFFFF">

<table border=0 cellpadding=0 cellspacing=0><tr><td bgcolor="#CCCC00"><table border=4 cellpadding=9><tr><td align=middle bgcolor="#000000"><font face="Palatino,Times" size=6 color="#999900"><b>
HMM & HMM: Get cross-entropy...
</b></font></table></table>
<p>Calculates the cross-entropy between the two selected <a href="HMM.html">HMM</a> models based on observation sequences.</p>
<h2>Settings</h2>
<dl>
<dt><b>Observation length</b>
<dd>defines the number of observations that have to generated.</dd>
<dt><b>Symmetric</b>
<dd>defines whether the symmetric formula is used in the calculation.</dd>
</dl>
<h2>Algorithm</h2>
<p>The cross-entropy is a measure of the distance between two models  &#955;<sub>1</sub> and &#955;<sub>2</sub>. It is defined as</p>
<table width="100%" style="white-space:pre-wrap"><tr><td align=middle><i>D</i>(&#955;<sub>1</sub>,&#955;<sub>2</sub>) = 1/<i>N</i> (log <i>p</i>(<i>O</i><sub>2</sub>|&#955;<sub>1</sub>) - log <i>p</i>(<i>O</i><sub>2</sub>|&#955;<sub>2</sub>)),</table>
<p>where <i>O</i><sub>2</sub> is an observation sequence of length  <i>N</i> generated by model  &#955;<sub>2</sub>.</p>
<p>The symmetrized version is:</p>
<table width="100%" style="white-space:pre-wrap"><tr><td align=middle><i>D</i><sub><i>s</i></sub>(&#955;<sub>1</sub>,&#955;<sub>2</sub>) = (<i>D</i>(&#955;<sub>1</sub>,&#955;<sub>2</sub>) + <i>D</i>(&#955;<sub>2</sub>,&#955;<sub>1</sub>))/2.</table>
<h3>Links to this page</h3>
<ul>
<li><a href="HMM___HMM___HMMObservationSequence__Get_cross-entropy.html">HMM & HMM & HMMObservationSequence: Get cross-entropy</a>
</ul>
<hr>
<address>
	<p>Â© djmw 20101017</p>
</address>
</body>
</html>
